# Content
- [shortcut learning in deep neural networks](https://github.com/YHJYH/Machine_Learning/blob/main/projects/Master_Thesis/papers/shortcut_learning_in_deep_NN.md#shortcut-learning-in-deep-neural-networks)
- [ImageNet-trained CNNs are biased towards texture; Increasing shape bias improves accuracy and robustness](https://github.com/YHJYH/Machine_Learning/blob/main/projects/Master_Thesis/papers/bias_towards_texture.md#imagenet-trained-cnns-are-biased-towards-texture-increasing-shape-bias-improves-accuracy-and-robustness)
- [Adversarial Examples are not Bugs, they are Features](https://github.com/YHJYH/Machine_Learning/blob/main/projects/Master_Thesis/papers/adversarial_examples_are_features.md#adversarial-examples-are-not-bugs-they-are-features)
- [Invariant Risk Minimization](https://github.com/YHJYH/Machine_Learning/blob/main/projects/Master_Thesis/papers/IRM.md#invariant-risk-minimization)
- [Similarity of Neural Network Representations Revisited (CKA)](https://github.com/YHJYH/Machine_Learning/blob/main/projects/Master_Thesis/papers/similarity_of_NN_CKA.md#similarity-of-neural-network-representations-revisited)
- [Learning De-biased Representations with Biased Representations](https://github.com/YHJYH/Machine_Learning/blob/main/projects/Master_Thesis/papers/learn_debias.md#learning-de-biased-representations-with-biased-representations)
- [RAPID LEARNING OR FEATURE REUSE? TOWARDS UNDERSTANDING THE EFFECTIVENESS OF MAML (remove inner loop)](https://github.com/YHJYH/Machine_Learning/blob/main/projects/Master_Thesis/papers/remove_inner_loop.md#rapid-learning-or-feature-reuse-towards-understanding-the-effectiveness-of-maml)
- [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://github.com/YHJYH/Machine_Learning/blob/main/projects/Master_Thesis/papers/lottery_ticket.md#the-lottery-ticket-hypothesis-finding-sparse-trainable-neural-networks)
- [The Pitfalls of Simplicity Bias in Neural Networks](https://github.com/YHJYH/Machine_Learning/blob/main/projects/Master_Thesis/papers/pitfall.md#the-pitfalls-of-simplicity-bias-in-neural-networks)
- [Gradient Starvation: A Learning Proclivity in Neural Networks](https://github.com/YHJYH/Machine_Learning/blob/main/projects/Master_Thesis/papers/gradient_starvation.md#gradient-starvation-a-learning-proclivity-in-neural-networks)


Out-of-distribution generalisation in deep neural networks:
- [Measuring the tendency of CNNs to Learn Surface Statistical Regularities](https://github.com/YHJYH/Machine_Learning/blob/main/projects/Master_Thesis/papers/Surface_Statistical_Regularities.md#measuring-the-tendency-of-cnns-to-learn-surface-statistical-regularities) 
- [On Calibration of Modern Neural Networks](https://github.com/YHJYH/Machine_Learning/blob/main/projects/Master_Thesis/papers/temp_scaling.md#on-calibration-of-modern-neural-networks) 始祖工作 be used to assess the uncertainty　associated with assigning the example to each of the classes. used **temperature scaling**
- [Enhancing the reliabilityof out-of-distribution image detection in neuralnetworks](https://github.com/YHJYH/Machine_Learning/blob/main/projects/Master_Thesis/papers/ODIN.md#enhancing-the-reliability-of-out-of-distribution-image-detection-in-neural-networks) another way to detect ood images.
- [A baseline for detecting misclassified and out-of-distribution examples in neural networks](https://github.com/YHJYH/Machine_Learning/blob/main/projects/Master_Thesis/papers/miclassied_or_ood.md#a-baseline-for-detecting-misclassified-and-out-of-distribution-examples-in-neural-networks) output confidence score to show if an example is abnormal, i.e. misclassified/out-of-distribution. used **temperature scaling**
- [The Origins and Prevalence of Texture Bias in Convolutional Neural Networks](https://github.com/YHJYH/Machine_Learning/blob/main/projects/Master_Thesis/papers/texture_bias.md#the-origins-and-prevalence-of-texture-bias-in-convolutional-neural-networks) texture bias
- [Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey](https://github.com/YHJYH/Machine_Learning/blob/main/projects/Master_Thesis/papers/adversarial_attacks_survey.md#threat-of-adversarial-attacks-on-deep-learning-in-computer-vision-a-survey) adversarial attacks
- [Partial success in closing the gap between human and machine vision](https://github.com/YHJYH/Machine_Learning/blob/main/projects/Master_Thesis/papers/close_gap.md#partial-success-in-closing-the-gap-between-human-and-machine-vision)
- [Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations](https://github.com/YHJYH/Machine_Learning/blob/main/projects/Master_Thesis/papers/surface_variation_robustness.md#benchmarking-neural-network-robustness-to-common-corruptions-and-surface-variations) noise data



Shortcuts:
- [DEEP LEARNING GENERALIZES BECAUSE THE PARAMETER-FUNCTION MAP IS BIASED TOWARDS SIMPLE FUNCTIONS](https://github.com/YHJYH/Machine_Learning/blob/main/projects/Master_Thesis/papers/PARAMETER_FUNCTION_MAP_IS_BIASED_TOWARDS_SIMPLE_FUNCTIONS.md#deep-learning-generalizes-because-the-parameter-function-map-is-biased-towards-simple-functions) why regularization works for DNNs to generaliza well.
- [Measuring the tendency of CNNs to Learn Surface Statistical Regularities](https://github.com/YHJYH/Machine_Learning/blob/main/projects/Master_Thesis/papers/CNN_learns_Surface_Statistical_Regularities.md#measuring-the-tendency-of-cnns-to-learn-surface-statistical-regularities)
- [Recognition in Terra Incognita](https://github.com/YHJYH/Machine_Learning/blob/main/projects/Master_Thesis/papers/Recognition_in_Terra_Incognita.md#recognition-in-terra-incognita) ood test dataset.


CNNs learn differently from human vision:
- [Intriguing properties of neural networks](https://github.com/YHJYH/Machine_Learning/blob/main/projects/Master_Thesis/papers/Intriguing_properties_of_NN.md#intriguing-properties-of-neural-networks)
- [Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images](https://github.com/YHJYH/Machine_Learning/blob/main/projects/Master_Thesis/papers/Deep_NNs_are_Easily_Fooled.md#deep-neural-networks-are-easily-fooled-high-confidence-predictions-for-unrecognizable-images)
- [Manitest: Are classifiers really invariant?](https://github.com/YHJYH/Machine_Learning/blob/main/projects/Master_Thesis/papers/manitest.md#manitest-are-classifiers-really-invariant) invariance in data samples.
- [A study and comparison of human and deep learning recognition performance under visual distortions](https://github.com/YHJYH/Machine_Learning/blob/main/projects/Master_Thesis/papers/comparison_of_human_and_DL_recognition_performance.md#a-study-and-comparison-of-human-and-deep-learning-recognition-performance-under-visual-distortions)
